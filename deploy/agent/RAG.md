ä»¥ä¸‹æ˜¯ä½¿ç”¨DeepSeekä½œä¸ºåŸºåº§æ¨¡å‹æ­å»ºRAGï¼ˆRetrieval-Augmented Generationï¼‰åº”ç”¨çš„è¯¦ç»†æ­¥éª¤ï¼Œåˆ†ä¸º7ä¸ªæ ¸å¿ƒé˜¶æ®µï¼š

---

### **ä¸€ã€ç¯å¢ƒå‡†å¤‡**
1. **ç¡¬ä»¶é…ç½®**
   - GPUæœåŠ¡å™¨ï¼ˆæ¨èNVIDIA A100/A800ï¼Œæ˜¾å­˜â‰¥40GBï¼‰
   - CPUï¼šå¤šæ ¸å¤„ç†å™¨ï¼ˆâ‰¥16æ ¸ï¼‰
   - RAMï¼šâ‰¥64GB
   - å­˜å‚¨ï¼šSSDç¡¬ç›˜ï¼ˆâ‰¥1TBï¼Œç”¨äºå­˜å‚¨å‘é‡ç´¢å¼•ï¼‰

2. **è½¯ä»¶ä¾èµ–**
   ```bash
   # åŸºç¡€ç¯å¢ƒ
   conda create -n rag python=3.10
   conda activate rag

   # æ ¸å¿ƒåº“å®‰è£…
   pip install torch==2.1.0+cu121 -f https://download.pytorch.org/whl/torch_stable.html
   pip install transformers==4.33.0
   pip install sentence-transformers==2.2.2  # ç”¨äºæ–‡æœ¬åµŒå…¥
   pip install faiss-cpu==1.7.4  # æˆ–faiss-gpuï¼ˆéœ€CUDAç¯å¢ƒï¼‰
   pip install langchain==0.0.340  # æµç¨‹ç¼–æ’
   pip install datasets==2.14.5  # æ•°æ®å¤„ç†
   ```

3. **æ¨¡å‹è·å–**
   - ä¸‹è½½DeepSeekæ¨¡å‹ï¼ˆå‡è®¾ä½¿ç”¨`deepseek-7b-base`ï¼‰
   ```python
   from transformers import AutoTokenizer, AutoModelForCausalLM
   tokenizer = AutoTokenizer.from_pretrained("deepseek-ai/deepseek-7b-base")
   model = AutoModelForCausalLM.from_pretrained("deepseek-ai/deepseek-7b-base", device_map="auto")
   ```

---

### **äºŒã€æ•°æ®å‡†å¤‡**
1. **æ–‡æ¡£æ”¶é›†**
   - æ ¼å¼ï¼šPDF/TXT/HTML/Markdown
   - é¢†åŸŸï¼šæ ¹æ®åº”ç”¨åœºæ™¯é€‰æ‹©ï¼ˆå¦‚åŒ»ç–—/æ³•å¾‹/æŠ€æœ¯æ–‡æ¡£ï¼‰

2. **æ•°æ®é¢„å¤„ç†**
   ```python
   from langchain.text_splitter import RecursiveCharacterTextSplitter

   text_splitter = RecursiveCharacterTextSplitter(
       chunk_size=512,  # æ ¹æ®æ–‡æ¡£ç‰¹æ€§è°ƒæ•´
       chunk_overlap=64,
       separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ"]
   )
   documents = text_splitter.split_documents(raw_docs)
   ```

3. **åµŒå…¥ç”Ÿæˆ**
   ```python
   from sentence_transformers import SentenceTransformer
   encoder = SentenceTransformer('BAAI/bge-large-zh-v1.5')  # ä¸­æ–‡æ¨è

   embeddings = []
   for doc in documents:
       embedding = encoder.encode(doc.page_content, normalize_embeddings=True)
       embeddings.append(embedding)
   ```

---

### **ä¸‰ã€æ£€ç´¢ç³»ç»Ÿæ­å»º**
1. **å‘é‡æ•°æ®åº“æ„å»º**
   ```python
   import faiss
   import numpy as np

   dimension = 1024  # ä¸åµŒå…¥ç»´åº¦ä¸€è‡´
   index = faiss.IndexFlatIP(dimension)
   index.add(np.array(embeddings).astype('float32'))
   faiss.write_index(index, "my_vector_store.index")
   ```

2. **æ£€ç´¢ä¼˜åŒ–**
   - ä½¿ç”¨IVF-PQç´¢å¼•åŠ é€Ÿï¼š
   ```python
   quantizer = faiss.IndexFlatIP(dimension)
   index = faiss.IndexIVFPQ(quantizer, dimension, nlist=100, m=8, bits=8)
   index.train(embeddings)
   index.add(embeddings)
   ```

---

### **å››ã€æ¨¡å‹é›†æˆ**
1. **Promptå·¥ç¨‹**
   ```python
   RAG_PROMPT_TEMPLATE = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å’Œä½ çš„çŸ¥è¯†ï¼Œå›ç­”ç”¨æˆ·é—®é¢˜ï¼š
   ä¸Šä¸‹æ–‡ï¼š
   {context}

   é—®é¢˜ï¼š{question}
   ç­”æ¡ˆï¼š"""
   ```

2. **æ£€ç´¢-ç”Ÿæˆæµç¨‹**
   ```python
   def rag_pipeline(query, k=5):
       # 1. æŸ¥è¯¢ç¼–ç 
       query_embedding = encoder.encode(query)
       
       # 2. æ£€ç´¢
       D, I = index.search(np.array([query_embedding]), k)
       retrieved_docs = [documents[i] for i in I[0]]
       
       # 3. ä¸Šä¸‹æ–‡æ‹¼æ¥
       context = "\n".join([doc.page_content for doc in retrieved_docs])
       
       # 4. ç”Ÿæˆ
       prompt = RAG_PROMPT_TEMPLATE.format(context=context, question=query)
       inputs = tokenizer(prompt, return_tensors="pt").to(model.device)
       outputs = model.generate(**inputs, max_new_tokens=512)
       return tokenizer.decode(outputs[0], skip_special_tokens=True)
   ```

---

### **äº”ã€åº”ç”¨å¼€å‘**
1. **APIæœåŠ¡ï¼ˆFastAPIç¤ºä¾‹ï¼‰**
   ```python
   from fastapi import FastAPI
   app = FastAPI()

   @app.post("/ask")
   async def ask_question(question: str):
       return {"answer": rag_pipeline(question)}
   ```

2. **Webç•Œé¢ï¼ˆGradioç¤ºä¾‹ï¼‰**
   ```python
   import gradio as gr

   interface = gr.Interface(
       fn=rag_pipeline,
       inputs=gr.Textbox(label="è¾“å…¥é—®é¢˜"),
       outputs=gr.Textbox(label="ç”Ÿæˆç­”æ¡ˆ")
   )
   interface.launch()
   ```

---

### **å…­ã€ä¼˜åŒ–ç­–ç•¥**
1. **æ£€ç´¢å¢å¼º**
   - æ··åˆæ£€ç´¢ï¼šç»“åˆBM25ï¼ˆå…³é”®è¯ï¼‰ä¸å‘é‡æ£€ç´¢
   - é‡æ’åºï¼šä½¿ç”¨äº¤å‰ç¼–ç å™¨ä¼˜åŒ–ç»“æœæ’åº

2. **ç”Ÿæˆä¼˜åŒ–**
   - å‚æ•°è°ƒæ•´ï¼š`temperature=0.7`, `top_p=0.9`
   - çº¦æŸç”Ÿæˆï¼šé€šè¿‡`bad_words_ids`è¿‡æ»¤æ•æ„Ÿå†…å®¹

3. **ç¼“å­˜æœºåˆ¶**
   - Redisç¼“å­˜é«˜é¢‘æŸ¥è¯¢ç»“æœ
   - é¢„è®¡ç®—å¸¸è§é—®é¢˜çš„åµŒå…¥å‘é‡

---

### **ä¸ƒã€éƒ¨ç½²ä¸ç›‘æ§**
1. **å®¹å™¨åŒ–éƒ¨ç½²**
   ```dockerfile
   FROM nvidia/cuda:12.1.0-base
   COPY . /app
   WORKDIR /app
   RUN pip install -r requirements.txt
   CMD ["uvicorn", "main:app", "--host", "0.0.0.0", "--port", "8000"]
   ```

2. **ç›‘æ§æŒ‡æ ‡**
   - å“åº”å»¶è¿Ÿï¼ˆç›®æ ‡<3sï¼‰
   - æ˜¾å­˜åˆ©ç”¨ç‡ï¼ˆé˜ˆå€¼<90%ï¼‰
   - æ£€ç´¢å‡†ç¡®ç‡ï¼ˆäººå·¥è¯„ä¼°é‡‡æ ·ï¼‰

3. **æŒç»­æ”¹è¿›**
   - ç”¨æˆ·åé¦ˆæ”¶é›†ï¼ˆğŸ‘/ğŸ‘æŒ‰é’®ï¼‰
   - å®šæœŸæ›´æ–°çŸ¥è¯†åº“ï¼ˆå¢é‡ç´¢å¼•æ„å»ºï¼‰

---

### **å…³é”®æ³¨æ„äº‹é¡¹**
1. æ•°æ®å®‰å…¨ï¼šæ•æ„Ÿæ•°æ®éœ€è¿›è¡Œè„±æ•å¤„ç†
2. ç‰ˆæœ¬æ§åˆ¶ï¼šç»´æŠ¤æ¨¡å‹/ç´¢å¼•/ä»£ç çš„ç‰ˆæœ¬å¯¹åº”å…³ç³»
3. æ³•å¾‹åˆè§„ï¼šç”Ÿæˆå†…å®¹éœ€æ·»åŠ å…è´£å£°æ˜

é€šè¿‡ä»¥ä¸Šæ­¥éª¤å¯æ„å»ºå®Œæ•´çš„RAGç³»ç»Ÿï¼Œå®é™…éƒ¨ç½²æ—¶éœ€æ ¹æ®å…·ä½“åœºæ™¯è°ƒæ•´å‚æ•°å’Œç»„ä»¶é€‰å‹ã€‚